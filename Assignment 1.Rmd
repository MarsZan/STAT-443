---
title: 'STAT 443: Assignment 1'
author: 'Wenxuan Zan (61336194)'
date: "2 Februray, 2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#if you do not have the package, type install.packages("name_of_the_package")
library(tidyverse)
library(knitr)
library(tseries)
library(zoo)
```

### Question 1

a)
```{r}
co2 <- read.csv("co2_mm_gl.csv", header = TRUE, skip = 55)
co2_ts <- ts(co2[,4], start = c(1979,1),frequency = 12)
plot(co2_ts,
     main = "Monthly Means of Globally Averaged CO2 Level",
     ylab = "CO2 level")
plot(window(co2_ts, start = c(1985,1),end = c(1986,1)),
     main = "CO2 Level Variation Within 12 Months",
     ylab = "CO2 level")
```

i) The above time series have a clear **upward trend** such that the monthly means of globally averaged CO2 level is increasing every year despite some variations.

ii) There appears to be **seasonal variations** in the monthly CO2 level as when we restrict the plot to display the average CO2 level over a 12-month period, we can clear see the CO2 level is high around March and December, low around June.**An additive model is more suitable** as we can see the seasonal effect remains constant over time and the error is also constant over time, therefore an additive model would be more appropriate than a multiplicative model. 

iii) **No**, the series have a clear upward increasing trend therefore it is not stationary. 

b)
```{r}
co2_train <- window(co2_ts,start = c(1979,1),end = c(2019,12),frequency = 12)
co2_test <- window(co2_ts, start = c(2020,1), end = c(2022,10), frequency = 12)

co2_train_decom <- decompose(co2_train, type = "additive")
plot(co2_train_decom)

co2_train_loess <- stl(co2_train,s.window = "periodic")
plot(co2_train_loess,
     main = "Decomposition of an Additive Time Series via Loess Smoothing")
```

c)
```{r}
# MA method
ma_trend <- co2_train_decom$trend
ma_seas <- co2_train_decom$seasonal
ma_error <- co2_train_decom$random
# Loess smoothing
loess_trend <- co2_train_loess$time.series[,"trend"]
loess_seas <- co2_train_loess$time.series[,"seasonal"]
loess_error <- co2_train_loess$time.series[,"remainder"]
# creating data frame
lm_data <- data.frame(ma_trend = ma_trend,
                      loess_trend = loess_trend,
                      time = c(1:length(co2_train)))
# Fitted Models
lm_ma <- lm(ma_trend~time, data = lm_data)
lm_loess <- lm(loess_trend~time, data = lm_data)
summary(lm_ma)
summary(lm_loess)
```

i) Fitted model using MA method: 
$$\hat{m_t} = 0.03329 + 0.1488*t$$

Fitted model using loess smoothing method:
$$\hat{m_t} = 0.03329 + 0.1491*t$$

ii) The trend component is significant at 95% confidence level under both method. 

iii) I think the trend component is a good predictor because the trend is linear over the entire time range of the time series data looking at the trend component plot in part b); second, from the output of the linear models we observe that $R^2 > 0.99$f ro both models. This indicates that majority of the variation within the time series is explained by the trend component. 

d)
```{r}
t_test <- data.frame(time = max(lm_data$time) + c(1:length(co2_test)))
ma_trend_pred <- predict(lm_ma, newdata = t_test)
loess_trend_pred <- predict(lm_loess, newdata = t_test)

# estimate seasonal effect 
seasonals <- tibble(ma_sea_hat = ma_seas,
                        loess_sea_hat = loess_seas,
                        season = rep(1:12,41)) %>%
  group_by(season) %>%
  summarise(shat_ma = mean(ma_sea_hat),
            shat_loess = mean(loess_sea_hat)) 

ma_pred <- ts(ma_trend_pred + seasonals$shat_ma, 
              start = c(2020,1), 
              frequency = 12)
loess_pred <- ts(loess_trend_pred + seasonals$shat_loess,
                 start = c(2020,1),
                 frequency = 12)
plot(co2_test, ylim = c(min(co2_test,ma_pred,loess_pred), max(co2_test,ma_pred,loess_pred)))
lines(ma_pred,col = "red")
lines(loess_pred, col = "blue")
legend(2020,
       418,
       legend = c("Test data", "MA Prediction", "Loess Prediction"),
       lty = c("solid","solid","solid"),
       col = c("black", "red","blue"))
# MSPE
MSPE <- tibble(ma_pred = ma_pred,
               loess_pred = loess_pred,
               observed = co2_test) %>%
  mutate(sqr_diff_ma = (observed - ma_pred)^2,
         sqr_diff_loess = (observed - loess_pred)^2) %>%
  summarise(mspe_ma = mean(sqr_diff_ma),
            mspe_loess = mean(sqr_diff_loess))
MSPE
```

Looking at the above table, the loess smoothing method result in lower MSPE on the test data. Therefore I would recommend to use the prediction model from the loess smoothing decomposition.



